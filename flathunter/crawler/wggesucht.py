"""Expose crawler for WgGesucht"""
import re
import time
import random
from typing import Optional, List, Dict, Any, Union

import requests
from bs4 import BeautifulSoup, Tag

from flathunter.logging import logger
from flathunter.abstract_crawler import Crawler


def get_title(title_row: Tag) -> str:
    """Parse the title from the expose title element"""
    return title_row.text.strip()


def get_url(title_row: Tag) -> Optional[str]:
    """Parse the expose URL from the expose title element"""
    a_element = title_row.find('a')
    if not isinstance(a_element, Tag) \
            or not a_element.has_attr('href') \
            or not isinstance(a_element.attrs['href'], str):
        return None
    return 'https://www.wg-gesucht.de/' + a_element.attrs['href'].removeprefix("/")


def extract_href_style(row: Tag) -> Optional[str]:
    """Extract the style attribute from a image div"""
    div = row.find('div', {"class": "card_image"})
    if not isinstance(div, Tag):
        return None
    a_element = div.find('a')
    if not isinstance(a_element, Tag) or not a_element.has_attr('style'):
        return None
    style = a_element.attrs['style']
    if not isinstance(style, str):
        return None
    return style


def get_image_url(row: Tag) -> Optional[str]:
    """Parse the image url from the expose"""
    try:
        href_style = extract_href_style(row)
        if href_style is None:
            return None
        image_match = re.match(r'background-image: url\((.*)\);', href_style)
        if image_match is None:
            logger.debug("Image style attribute found but URL pattern not matched: %s", href_style[:100])
            return None
        return image_match[1]
    except Exception as e:
        logger.debug("Error extracting image URL: %s", str(e))
        return None


def get_rooms(row: Tag) -> str:
    """Parse the number of rooms from the expose"""
    try:
        details_el = row.find("div", {"class": "col-xs-11"})
        if not isinstance(details_el, Tag):
            return ""
        detail_string = details_el.text.strip().split("|")
        details_array = list(map(lambda s: re.sub(' +', ' ',
                                                  re.sub(r'\W', ' ', s.strip())),
                                 detail_string))
        if not details_array or len(details_array) == 0:
            return ""
        rooms_tmp = re.findall(r'\d Zimmer', details_array[0])
        return rooms_tmp[0][:1] if rooms_tmp else ""
    except (IndexError, AttributeError) as e:
        logger.debug("Error parsing room count: %s", str(e))
        return ""
    except Exception as e:
        logger.debug("Unexpected error parsing room count: %s", str(e))
        return ""


def get_price(numbers_row: Tag) -> Optional[str]:
    """Parse the price from the expose"""
    price_el = numbers_row.find("div", {"class": "col-xs-3"})
    if not isinstance(price_el, Tag):
        return None
    return price_el.text.strip()


def get_dates(numbers_row: Tag) -> List[str]:
    """Parse the advert dates from the expose"""
    date_el = numbers_row.find("div", {"class": "text-center"})
    if not isinstance(date_el, Tag):
        return []
    return re.findall(r'\d{2}.\d{2}.\d{4}', date_el.text)


def get_size(numbers_row: Tag) -> List[str]:
    """Parse the room size from the expose"""
    size_el = numbers_row.find("div", {"class": "text-right"})
    if not isinstance(size_el, Tag):
        return []
    return re.findall(r'\d{1,4}\smÂ²', size_el.text)

def is_verified_company(row: Tag) -> bool:
    """Filter out ads from 'Verified Companies'"""
    verified_el = row.find("span", {"class": "label_verified"})
    if isinstance(verified_el, Tag):
        return True
    return False

# pylint: disable=too-many-return-statements
def parse_expose_element_to_details(row: Tag, crawler: str) -> Optional[Dict]:
    """Parse an Expose soup element to an Expose details dictionary"""
    url = "unknown"
    try:
        title_row = row.find('h2', {"class": "truncate_title"})
        if title_row is None or not isinstance(title_row, Tag):
            logger.warning("No title element found in listing - skipping (page structure may have changed)")
            return None
        if is_verified_company(row):
            logger.debug("Verified company listing detected - skipping")
            return None
        title = get_title(title_row)
        url = get_url(title_row)
        if url is None:
            logger.warning("No expose URL found for listing '%s' - skipping", title)
            return None
        image = get_image_url(row)
        if image is None:
            logger.debug("No image URL found for expose %s", url)
        rooms = get_rooms(row)
        if not rooms:
            logger.debug("No room count found for expose %s", url)
        numbers_row = row.find("div", {"class": "middle"})
        if not isinstance(numbers_row, Tag):
            logger.warning("No numbers section found for expose %s - skipping (page structure may have changed)", url)
            return None
        price = get_price(numbers_row)
        if price is None:
            logger.debug("No price found for expose %s", url)
        dates = get_dates(numbers_row)
        if len(dates) == 0:
            logger.warning("No availability dates found for expose %s - skipping", url)
            return None
        size = get_size(numbers_row)
        if len(size) == 0:
            logger.warning("No size information found for expose %s - skipping", url)
            return None

        if len(dates) == 2:
            title = f"{title} vom {dates[0]} bis {dates[1]}"
        else:
            title = f"{title} ab dem {dates[0]}"

        details = {
            'id': int(url.split('.')[-2]),
            'image': image,
            'url': url,
            'title': title,
            'price': price,
            'size': size[0],
            'rooms': rooms,
            'address': url,
            'crawler': crawler
        }
        if len(dates) == 2:
            details['from'] = dates[0]
            details['to'] = dates[1]
        elif len(dates) == 1:
            details['from'] = dates[0]

        logger.debug("Successfully parsed expose: %s", title)
        return details

    except (ValueError, IndexError) as e:
        logger.error("Error parsing expose ID from URL %s: %s", url, str(e))
        return None
    except Exception as e:
        logger.error("Unexpected error parsing expose %s: %s", url, str(e), exc_info=True)
        return None


def liste_attribute_filter(element: Union[Tag, str]) -> bool:
    """Return true for elements whose 'id' attribute starts with 'liste-' 
    and are not contained in the 'premium_user_extra_list' container"""
    if not isinstance(element, Tag):
        return False
    if not element.attrs or "id" not in element.attrs:
        return False
    if not element.parent or not element.parent.attrs or "class" not in element.parent.attrs:
        return False
    return element.attrs["id"].startswith('liste-') and \
        'premium_user_extra_list' not in element.parent.attrs["class"]


class WgGesucht(Crawler):
    """Implementation of Crawler interface for WgGesucht"""

    URL_PATTERN = re.compile(r'https://www\.wg-gesucht\.de')

    def __init__(self, config):
        super().__init__(config)
        self.config = config

    # pylint: disable=too-many-locals
    def extract_data(self, raw_data: BeautifulSoup) -> List[Dict]:
        """Extracts all exposes from a provided Soup object"""
        entries = []

        try:
            findings = raw_data.find_all(liste_attribute_filter)
            if not findings:
                logger.warning("No listing elements found on page - page structure may have changed or no results available")
                return entries

            existing_findings = [
                e for e in findings
                if isinstance(e, Tag) and e.has_attr('class') and not 'display-none' in e['class']
            ]

            logger.debug('Found %d total listings (%d visible)', len(findings), len(existing_findings))

            for row in existing_findings:
                details = parse_expose_element_to_details(row, self.get_name())
                if details is None:
                    continue
                entries.append(details)

            logger.debug('Successfully extracted %d out of %d WG-Gesucht listings', len(entries), len(existing_findings))

        except Exception as e:
            logger.error("Error extracting data from WG-Gesucht page: %s", str(e), exc_info=True)

        return entries

    def load_address(self, url) -> Optional[str]:
        """Extract address from expose itself"""
        try:
            response = self.get_soup_from_url(url)
            if not response:
                logger.warning("Failed to load page for address extraction: %s", url)
                return None

            address_div = response.find('div', {"class": "col-sm-4 mb10"})
            if not isinstance(address_div, Tag):
                logger.debug("No address container found in expose page: %s", url)
                return None

            a_element = address_div.find("a", {"href": "#mapContainer"})
            if not isinstance(a_element, Tag):
                logger.debug("No address link found in expose page: %s", url)
                return None

            address = ' '.join(a_element.text.strip().split())
            logger.debug("Successfully extracted address for %s: %s", url, address)
            return address

        except Exception as e:
            logger.error("Error loading address from %s: %s", url, str(e), exc_info=True)
            return None

    def get_soup_from_url(
            self,
            url: str,
            driver: Optional[Any] = None,
            checkbox: bool = False,
            afterlogin_string: Optional[str] = None,
            retry_count: int = 0) -> BeautifulSoup:
        """
        Creates a Soup object from the HTML at the provided URL

        Overwrites the method inherited from abstract_crawler. This is
        necessary as we need to reload the page once for all filters to
        be applied correctly on wg-gesucht.

        Implements exponential backoff with jitter for retries to avoid rate limiting.

        Args:
            url: URL to fetch
            driver: Optional Selenium driver
            checkbox: Captcha checkbox flag
            afterlogin_string: String to wait for after login
            retry_count: Current retry attempt (for internal use)
        """
        max_retries = 3
        base_delay = 5  # seconds

        try:
            sess = requests.session()
            # First page load to set filters; response is discarded
            logger.debug("Loading WG-Gesucht page (first request to set filters): %s", url)
            sess.get(url, headers=self.HEADERS, timeout=30)
            # Second page load
            logger.debug("Loading WG-Gesucht page (second request): %s", url)
            resp = sess.get(url, headers=self.HEADERS, timeout=30)

            # Check for rate limiting (HTTP 429) or server errors (5xx)
            if resp.status_code == 429:
                logger.warning("WG-Gesucht rate limit hit (HTTP 429)")
                raise requests.exceptions.RequestException("Rate limited")
            elif resp.status_code >= 500:
                logger.warning(f"WG-Gesucht server error (HTTP {resp.status_code})")
                raise requests.exceptions.RequestException("Server error")
            elif resp.status_code not in (200, 405):
                logger.error("Received HTTP %d from WG-Gesucht for URL %s: %s",
                           resp.status_code, url, resp.content[:200])
            elif resp.status_code == 200:
                logger.debug("Successfully loaded page: %s", url)

            if self.config.use_proxy():
                logger.debug("Using proxy for URL: %s", url)
                return self.get_soup_with_proxy(url)
            if driver is not None:
                logger.debug("Using webdriver for URL: %s", url)
                driver.get(url)
                if re.search("initGeetest", driver.page_source):
                    logger.info("Geetest captcha detected, attempting to resolve...")
                    self.resolve_geetest(driver)
                elif re.search("g-recaptcha", driver.page_source):
                    logger.info("reCAPTCHA detected, attempting to resolve...")
                    self.resolve_recaptcha(
                        driver, checkbox, afterlogin_string or "")
                return BeautifulSoup(driver.page_source, 'lxml')
            return BeautifulSoup(resp.content, 'lxml')

        except (requests.exceptions.Timeout, requests.exceptions.RequestException) as exc:
            if retry_count >= max_retries:
                logger.error(
                    f"WG-Gesucht page load failed after {max_retries} retries - likely rate limited or down: {url}"
                )
                # Return empty soup after exhausting retries
                return BeautifulSoup("", 'lxml')

            # Exponential backoff with jitter to avoid thundering herd
            delay = base_delay * (2 ** retry_count) + random.uniform(0, 3)
            logger.warning(
                f"WG-Gesucht request failed ({type(exc).__name__}), "
                f"retrying in {delay:.1f}s (attempt {retry_count + 1}/{max_retries})"
            )
            time.sleep(delay)

            return self.get_soup_from_url(url, driver, checkbox, afterlogin_string, retry_count + 1)

        except Exception as e:
            logger.error("Unexpected error loading WG-Gesucht page %s: %s", url, str(e), exc_info=True)
            return BeautifulSoup("", 'lxml')
